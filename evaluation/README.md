## ðŸ“‚ Evaluation

This folder contains all files used to test and validate the recommendation system through two fan surveys.

### ðŸ”¹ Python scripts
- **analyze_responses.py** â€” maps raw survey text responses to structured `game_id`s for evaluation.  
- **evaluate_responses.py** â€” computes Recall@k and NDCG@k metrics for the first survey.  
- **evaluate_responses2.py** â€” same as above, applied to the second survey.  
- **compare_evaluations.py** â€” compares evaluation outputs across methods and survey runs.  
- **run_simple.py** â€” quick script for running baseline or simplified evaluations.

### ðŸ”¹ Survey data
- **survey_responses.csv** â€” raw responses from the first fan survey.  
- **survey_responses2.csv** â€” raw responses from the second fan survey.  
- **survey_responses_with_ids.csv** â€” processed first survey (responses mapped to `game_id`s).  
- **survey_responses_with_ids2.csv** â€” processed second survey (mapped to `game_id`s).

### ðŸ”¹ Evaluation outputs
- **compare_methods.csv** â€” comparison of different recommendation methods (survey 1).  
- **compare_methods2.csv** â€” comparison of methods (survey 2).  
- **compare_methods_comb.csv** â€” combined comparison across both surveys.  
- **evaluation_metrics.csv** â€” computed metrics for survey 1 (main method).  
- **evaluation_metrics2.csv** â€” computed metrics for survey 2 (main method).  
- **evaluation_metrics_exc.csv** â€” evaluation using excitement-only baseline (survey 1).  
- **evaluation_metrics_exc2.csv** â€” same, for survey 2.  
- **evaluation_metrics_max.csv** â€” evaluation using max-similarity baseline (survey 1).  
- **evaluation_metrics_max2.csv** â€” same, for survey 2.  
- **evaluation_metrics_rnd.csv** â€” random baseline metrics (survey 1).  
- **evaluation_metrics_rnd2.csv** â€” random baseline metrics (survey 2).  
- **metrics_comparison.svg** â€” visualization comparing metrics across methods and surveys.  

